Description of included files:


SOURCE CODE and HEADER FILES

ParallelizedAdd.c ------------------------------------

C Source Code file that creates numerous threads, where each thread modifies a global integer variable. 

SortedList.h ------------------------------------

C header file which outlines the specifications for a LinkedList with insert, lookup, delete, and length functions. 

SortedList.c ------------------------------------

C source code file which implements the specifications from SortedList.h, implementing the designated four functions. 

ParallelizedList.c ------------------------------------

C source code file which builds on SortedList.c to create numerous threads. 
From there, each thread adds and deletes from a global linkedlist. 

SCRIPT FILE

test.sh ------------------------------------

Bash script to create graph data from ParallelizedAdd.c and ParallelizedList.c source code files. 
The data generated by this test script then goes into the following csv files:

MAKEFILE

Makefile -----------------------------------

default case: build lab2_add and lab2_list executables with the -Wall, -Wextra parameters.
tests: runs the test script
graphs: tests dependent. Generates graphs from output of tests
clean: deletes all files created by the Makefile, and returns the directory to its freshly untared state.
dist: builds the distribution tarball


The errors in threading take place when an interrupt occurs in the middle of some register load or read. When this happens, some global value can be accessed by two threads concurrently, which may not be ideal. However, when the number of iterations is low, it is more likely that the thread will finish running its course without an interrupt, in which case our program does not get corrupted. This is the reason why the program corrupts so easily with a large number of iterations, and so rarely with a smaller number of iterations. 

The yield runs are considerably slower because we essentially force an interrupt, which forces a context switch, saving of registers, etc. This is where the additional time goes. Because it is not easy to acquire the time lost to the context switch, it is very difficult to get a good approximation of valid per-operation timings if we are using the --yield option. 

The time I am counting in my program is from the creation of my threads all the way to the joining of our threads. In this process, there are numerous procedure and system calls made, which produces quite a large time overhead. Compared to this overhead, the cost of running iterations in a loop is considerably smaller. Therefore, the larger our ratio of iterations:overhead, the more efficient our program is time-wise. This is why the average cost per operation drops with increasing iterations. To obtain the "true" cost per iteration, we would have to run our program with an immensely large number of iterations, so that the ratio of iterations:overhead is infinitely large. 

Because the locks are only activated/useful when our program is interrupted, all locks perform similarly on a small number of iterations. This is because there are very few interrupts in the middle of a small number of iterations. These locks also slow down considerably with a larger number of iterations, because the number of interrupts/collisions rises considerably. Because the locks all implement some form of waiting mechanism, we are essentially waiting for threads to finish during through interrupts so we can avoid collisions. 

The per mutex protected operations increase as the number of threads increase because the number of potential interrupts that a thread can be caught in increases as well. Therefore, the lock is called into action far more times than otherwise. Additionally, the lab2_list program shows a faster rate of growth, because there are more protected operations that a thread can potentially get interrupted in. Therefore, there is a larger need for locking mechanisms. 

Between the mutex and spinning lock mechanisms, there seems to be a crossover point where the mutex becomes faster. This seems to be around 8 threads, but of course that is native to this program. In general, the mutex locking mechanism would be faster for large numbers of threads because the CPU time is being allotted to some other worthwhile process, instead of simply spinning and waiting for another thread to complete.